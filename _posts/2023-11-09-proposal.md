---
layout: distill
title: Graph neural networks v.s. transformers for geometric graphs
description: With the recent development of graph transformers, in this project we aim to compare their performance on a molecular task of protein-ligand binding affinity prediction against the performance of message passing graph neural networks.
date: 2023-11-01
htmlwidgets: true

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Ada Fang
    affiliations:
      name: MIT

# must be the exact same name as your blogpost
bibliography: 2023-11-09-proposal.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Introduction
  - name: Relevant work
    subsections:
    - name: Graph neural networks 
    - name: Graph transformers
  - name: Problem definition
  - name: Dataset
  - name: Proposed experiments
    subsections:
    - name: Proposed algorithmic contributions
    - name: Can transformers better capture long range interactions
    - name: Can graph neural networks approximate transformers with a fully connected graph

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction
Machine learning on graphs is often approached with message passing graph neural network (GNN) models where nodes in the graph are embedded with aggregated messages passed from neighboring nodes <d-cite key=zhou2020graph></d-cite>. However, with the significant success of transformers in language modelling <d-cite key=vaswani2017attention></d-cite> and computer vision recently <d-cite key=dosovitskiy2020image></d-cite>, there are a growing number of transformers developed for graphs as well. In this project we investigate the application of graph neural networks compared to transformers on geometric graphs defined on point clouds. We aim to explore the performance of these two models on predicting the binding affinity for a protein-ligand interaction given the atomic coordinates of the docked protein-ligand structure, which is a highly relevant task in drug discovery. This blog post is divided into four parts: an introduction into graph neural networks and transformers on point clouds, our model architecture, experimental results, and a discussion comparing the two architectures.

## Background and relevant work
### Graph neural networks on molecules
Graphs are comprised of nodes and edges, and we can model any set of objects with a defined connectivity between the as a graph. For example, social networks are a set of people and the connectivity between them is based on who knows who. We can also think of structured data like an image as a graph where each pixel is a node and edges are defined to the adjacent pixels. Any sequential data, such as text, can be modeled as a graph of connected words. In this section we focus on graphs of molecules where edges are defined between atoms. These edges are often defined by the molecular bonds or for atoms with 3D coordinate information we can use a spatial cutoff $d$ based on the Euclidean distance between nodes. Given a graph we can use a graph neural network (GNN) to learn a meaningful representation of the graph and use these representations for predictive tasks such as node-level prediction, edge-level prediction, or graph-level prediction. Graph neural networks learn thorugh successive layers of message passing between nodes and their neighbouring nodes. 

An important property of many GNNs applied on 3D molecules is SE(3)-equivariance. This means that any transformation of the input in the SE(3) symmetry group which includes all rigid body translations and rotations in $\mathbb{R}^3$ will result in the same transformation applied to the output. This property is important for the modelling of physical systems; for example if the prediction task is the force applied on an atom in a molecule rotation of the molecule should result in the model predicting the same forces but rotated. In some tasks we do not need equivariance but rather SE(3)-invariance which is a subset of SE(3)-equivariance, where any transformation of the input in the SE(3) symmetry group results in the same output. This is often the case when the task of the model is to predict a global property of the molecule which should not change if all 3D coordinates of the molecule are translated and rotated. SE(3)-invariance will be required for our model of binding affinity as global rotations and translations of the protein-ligand structure should yield the same predicted binding affinity.

Early SE(3)-equivariant GNNs on point clouds used directional message passing <d-cite key="gasteiger2020directional"></d-cite> which used the pairwise distance and direction between nodes as features for the GNN, however they were soon shown to be limited in expressivity <d-cite key="garg2020generalization"></d-cite>. Now state-of-the-art (SOTA) models in this area are based on higher order geometric properties such as dihedral angles and representations in the geometric group SO(3). Some examples include GemNet  <d-cite key=gasteiger2021gemnet></d-cite> and e3nn <d-cite key=geiger2022e3nn></d-cite>. e3nn has also shown that it is much more data-efficient when learning as the model does not need to learn to be equivariant, which non-equivariant models do. These models have led to exceptional performance for tasks related to predicting molecular forces and energies <d-cite key=batzner20223></d-cite> <d-cite key=musaelian2023learning></d-cite>. For the task of binding affinity some GNNs that achieve high performance using GNNs are ProNet <d-cite key=wang2022learning></d-cite> and HoloProt <d-cite key=somnath2021multi></d-cite>.

### Graph transformers
With the proliferation of transformers in the broader field of machine learning, this has also influenced the development of graph transformers. In a transformer model each node attends to all other nodes in the graph via attention where the queries and values are projections of the feature vector of a node, and the key is the projection of feature vectors of all other nodes. Hence, graph transformers and transformers applied to sequences (e.g. text) are largely similar in architecture; however, differences arise in the positional encodings in a graph transformer as it is defined in relation to other nodes in the graph <d-cite key=ying2021transformers></d-cite>. For geometric graphs, positional encodings can be applied as a bias term on the attention value of node $u$ on $v$, where the bias is a learned value that is dependent on the distance between the nodes <d-cite key=zhou2023uni></d-cite> <d-cite key=luo2022one></d-cite>. There are also other ways of implementing positional encodings in the form of Laplacian eigenvectors, and random walk diagonals <d-cite key=rampavsek2022recipe></d-cite>. Recently, in an effort to unify different methods to generate structural and positional graph encodings, Liu et al. <d-cite key=liu2023graph></d-cite> apply a novel pretraining approach with a multiobjective task of learning a variety of positional and structural encodings to derive more general positional and structural encodings. Graph transformers are also achieving SOTA performance for benchmarks on predicting quantum properties of molecules <d-cite key=zhou2023uni></d-cite> <d-cite key=luo2022one></d-cite> and binding affinity <d-cite key=kong2023generalist></d-cite>.

## Motivation
Given the growing application of both GNNs and transformers we aim to compare their performance on the same task of protein-ligand binding affinity prediction. We also aim to compare models as we can see analogies between graph transformers and GNNs, where "message passing" in the graph transformer involves messages from all nodes rather than the local neighbourhood of nodes. We view protein-ligand binding affinity prediction as a suitable task to compare the two architectures as there are aspects of both the GNN and graph transformer architecture that would be advantageous for the task: binding affinity is a global prediction task for which the graph transformer may better capture global dependencies, although binding affinity is also driven by local structural orientations between the protein and ligand which the graph neural network may learn more easily.


## Problem definition
* The input to the model is a set of atoms for the protein pocket $X_{\mathrm{protein}}$ and ligand $X_{\mathrm{ligand}}$, for which we have the atomic identity and the 3D coordinates, and the binding affinity $y$ for the structure.
* For the graph neural network we define a molecular graph of the protein ligand structure $G=(V,E)$ where $V$ are the $n$ nodes that represent atoms in the molecule and the edges $E$ are defined between two nodes if their 3D distance is within a radial cutoff $r$. We further define two types of edges: intramolecular edges for edges between nodes within $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$, and intermolecular edges for nodes between $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$.
* For the graph transformer it is applied to the whole set of atoms $(X_{\mathrm{protein}}, X_{\mathrm{ligand}})$, and we can use the 3D coordinates of the atoms to derive positional encodings.
* Performance is determined by the root mean squared error, Pearson, and Spearman correlation coefficients between true binding affinity and predicted binding affinity.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2023-11-09-proposal/problem_definition.png" %}
    </div>
</div>
<div class="caption">
    A protein-ligand structure, Protein Data Bank (PDB) entry 1a0q. The protein backbone is shown in blue, and the ligand is shown in green. The model would be given this structure and the objective is to predict the binding affinity of the ligand to the protein. 
</div>

## Dataset
We use the PDBbind dataset for the protein-ligand structures and binding affinity. In addition, for benchmarking we use the benchmark from ATOM3D <d-cite key="townshend2020atom3d"></d-cite> with a 30% and 60% sequence identity split on the protein to better test generalisability of the model.

## Architecture
### Graph neural network

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2023-11-09-proposal/gnn_architecture1.png" %}
    </div>
</div>
<div class="caption">
    Overview of the GNN architecture for a graph constructed from a protein-ligand structure.
</div>

A graph is constructed from the atomic coordinates of the atoms in the protein pocket $X_{\mathrm{protein}}$ and ligand $X_{\mathrm{ligand}}$ where the nodes are the atoms. Intramolecular edges are defined between nodes within $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$ with a distance cutoff of 3 Å, and intermolecular edges for nodes between $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$ with a distance cutoff of 6 Å. The model architecture is defined as follows:

(1) Initial feature vectors of the nodes are based on a learnable embedding of their atomic elements. 

(2) We define two types of messages in the GNN, given by the two types of edges, intermolecular messages and intramolecular messages. The architecture used for the two types are messages are the same but the weights are not shared, this is to reflect that information transferred between atoms within the same molecule is chemically different to information transferred between atoms of different molecules. The message passing equation uses the tensor product network introduced by e3nn <d-cite key=geiger2022e3nn></d-cite>, and our implementation is based on the message passing framework used by DiffDock <d-cite key=corso2022diffdock></d-cite>. We omit the details of the tensor product network for simplicity but provide the overall method below.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2023-11-09-proposal/message_passing_eqn.png" %}
    </div>
</div>

where node $b$ are the neighbours of node $a$ in $G$ given by intermolecular or intramolecular edges denoted with $t$. The message is computed with tensor products between the spherical harmonic projection with rotation order $\lambda = 2$ of the unit bond direction vector, $$Y^{(\lambda)}({\hat{r}}_{a b})$$, and the irreps of the feature vector of the neighbour $h_b$. This is a weighted tensor product and the weights are given by a 2-layer MLP, $\Psi^{(t)}$ , based on the $\mathrm{0e}$ (scalar) features of the nodes $h_a$ and $h_b$ and the edge features $e_{ab}$. 

(3) After $k$ layers of message passing we perform pooling for the nodes of $X_{\mathrm{protein}}$ and the nodes of $X_{\mathrm{ligand}}$ by message passing to the "virtual nodes" defined by the centroid of the protein and ligand, using the same message passing framework outlined above.

(4) Finally, we concatenate the embedding of the centroid of the protein and ligand and pass this vector to a 3 layer MLP which outputs a singular scalar, the binding affinity prediction.

### Graph transformer

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2023-11-09-proposal/graph_transformer_architecture1.png" %}
    </div>
</div>
<div class="caption">
    Overview of the graph transformer architecture for a graph constructed from a protein-ligand structure.
</div>

The model architecture is as follows:

(1) Initial feature vectors of the nodes are based on a learnable embedding of their atomic elements. 

(2) The graph transformer architecture is based on graphormer <d-cite key=ying2021transformers></d-cite>. Where the input is $H \in \mathbb{R}^{n \times d}$ where $d$ is the hidden dimension and $n$ is the number of nodes. The input is projected by $W_Q \in \mathbb{R}^{d \times d_K}, W_K \in \mathbb{R}^{d \times d_K}, W_V \in \mathbb{R}^{d \times d_V}$. Since graphs are more complex relative information to capture than sequeunces, conventional positional encoding methods used in sequence-based transformers are not applicable to graphs. Positions in a graph are defined relative to all other nodes, thus positional embeddings cannot be added at the node feature vector level but instead are added as a bias to the attention matrix. We define $B \in \mathbb{R}^{n \times n}$, where $B_{ij}$ is given by a Gaussian basis embedding of the Euclidean distance $d_{ij}$ between node $i$ and $j$, which is passed to a 3 layer MLP that outputs a singular scalar. Then the self-attention is calculated as $Q = HW_Q, K = HW_K, V = HW_V$ and $A = \frac{QK^T + B}{\sqrt{d_k}}, Attn(H) = Softmax(A) V$. In addition to all atomic nodes, we also add a `<cls>` token used in the BERT model which functions as a virtual global node. The distance of this node to all other nodes is a learnable parameter. This process is duplicated across multiple heads and we concatenate the embeddings across all heads after $k$ layers as the updated feature vector.

(3) We take the final embedding of the `<cls>` node and pass it through a 3 layer MLP which outputs a singular scalar, the binding affinity prediction.

### Loss function
Both models are trained to minimise the root mean squared error between the predicted binding affinity and true binding affinity. 


## Experiments
In order for the results to be comparable between the two models, both models have approximately 2.8 million parameters.

GNN model details: 4 layers of message passing, number of scalar features = 32, number of vector features = 13. Graph transformer model details: 8 attention heads, 8 layers, hidden dimension = 192, feed forward neural network dimension = 512. Both models were trained for 12 hours on 1 GPU with a batch size of 16 and a learning rate of $1 \times 10^{-3}$. We show the results for the 30% and 60% sequence-based splits for the protein-ligand binding affinity benchmark in Table 1 and 2 respectively.

**Table 1.** Protein-ligand binding affinity task with 30% sequence based split.

| Model | Root mean squared error $\downarrow$ | Pearson correlation coefficient $\uparrow$ | Spearman correlation coefficient $\uparrow$ |
|----------|----------|----------|----------|
| Row 2, Col 1 | Row 2, Col 2 | Row 2, Col 3 | Row 2, Col 4 |
| Row 3, Col 1 | Row 3, Col 2 | Row 3, Col 3 | Row 3, Col 4 |
| Row 4, Col 1 | Row 4, Col 2 | Row 4, Col 3 | Row 4, Col 4 |

**Table 2.** Protein-ligand binding affinity task with 60% sequence based split.

| Model | Root mean squared error $\downarrow$ | Pearson correlation coefficient $\uparrow$ | Spearman correlation coefficient $\uparrow$ |
|----------|----------|----------|----------|
| Row 2, Col 1 | Row 2, Col 2 | Row 2, Col 3 | Row 2, Col 4 |
| Row 3, Col 1 | Row 3, Col 2 | Row 3, Col 3 | Row 3, Col 4 |
| Row 4, Col 1 | Row 4, Col 2 | Row 4, Col 3 | Row 4, Col 4 |


## Discussion
### Trying to learn from everything may lead to learning nothing
From the benchmarking we can see that the graph transformer model performs worse than the GNN across both the 30% and 60% sequence split for protein-ligand binding affinity. An intuitive explanation for why is that it may be difficult for the graph transformer to learn the importance of local interactions for binding affinity prediction as it attends to all nodes in the network. Or in other words, because each update of the node involves seeing all nodes, it can be difficult to decipher which nodes are important and which nodes are not. In order to test if this is true, future experiments would involve a graph transformer with a sparse attention layer where the attention for nodes beyond a distance cutoff is 0.

### More layers?
Adding on more layers to the GNN did result in much better performance.

### A hybrid approach: GNNs with Transformers
Potentially we could improve performance further by taking a hybrid approach. That is using the GNN to first learn local interactions and use a graph transformer to learn global interactions and pool the local node embeddings into a global binding affinity value. The motivation for this design is to leverage the advantages of both models.


We will implement two models, a SE(3)-equivariant graph neural network based on Tensor Field Networks using e3nn <d-cite key=geiger2022e3nn></d-cite> and DiffDock <d-cite key=corso2022diffdock></d-cite> (a protein-ligand docking model), and a graph transformer based on the architecture proposed by Transformer-M <d-cite key=luo2022one></d-cite>. For fair comparison we will ensure the number of trainable parameters in both models is comparable by adjusting the number of layers and embedding dimension. The models will be trained to convergence on the ATOM3D dataset split and the best performing model on the validation split will be used to evaluate the test split.

### Proposed algorithmic contributions
For the GNN we will use the confidence model in DiffDock <d-cite key=corso2022diffdock></d-cite> as an analogy to the binding affinity predictor model. The confidence model in DiffDock is given a docked protein-ligand structure and it scores how likely the structure is within 2 $\overset{\circ}{A}$ to the true structure. Similarly, the binding affinity model will be given the coordinates of the experimental protein-ligand structure and will predict the protein-ligand binding affinity.

For the transformer, Transformer-M <d-cite key=luo2022one></d-cite> is pretrained on a broad set of 2D and 3D molecular structures and has been finetuned to predict protein-ligand binding affinity. However, we would like to compare this to a GNN model in a fair way, which would require using the Transformer-M architecture for only the 3D structure input track and predicting binding affinity with only the training dataset.

### Can transformers better capture long range interactions
Fundamentally, transformers vary from graph neural networks with their ability to capture long range interactions compared to the $k$-hop neighbourhoods that can be captured by a $k$-layer graph neural network. We explore how model performance is a function of graph size and diameter for the two model archetypes to see if transformers are better at capturing long range interactions. We will also isolate subsets of molecules where the models achieve the best and worse performance to compare if the models are excelling in similar areas.

### Can graph neural networks approximate transformers with a fully connected graph
One of the fundamental differences between transformers and GNNs is the neighborhood of nodes that each node receives updates from. For a transformer this is all nodes in a graph, and for a GNN this is the $k$-hop neighborhood. To bridge these differences we can construct a fully connected graph by increasing the radial cutoff $r$ for edges in the graph. We want to test for a GNN trained on a fully connected graph if we would achieve similar performance to the graph transformer. 