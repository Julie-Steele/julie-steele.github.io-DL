@article{rag,
  author       = {Patrick S. H. Lewis and
                  Ethan Perez and
                  Aleksandra Piktus and
                  Fabio Petroni and
                  Vladimir Karpukhin and
                  Naman Goyal and
                  Heinrich K{\"{u}}ttler and
                  Mike Lewis and
                  Wen{-}tau Yih and
                  Tim Rockt{\"{a}}schel and
                  Sebastian Riedel and
                  Douwe Kiela},
  title        = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  journal      = {CoRR},
  volume       = {abs/2005.11401},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.11401},
  eprinttype    = {arXiv},
  eprint       = {2005.11401},
  timestamp    = {Fri, 29 May 2020 09:57:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-11401.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{mteb,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sbert,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{attn,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{nmt,
title = {Progress in Machine Translation},
journal = {Engineering},
volume = {18},
pages = {143-153},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S2095809921002745},
author = {Haifeng Wang and Hua Wu and Zhongjun He and Liang Huang and Kenneth Ward Church},
keywords = {Machine translation, Neural machine translation, Simultaneous translation},
abstract = {After more than 70 years of evolution, great achievements have been made in machine translation. Especially in recent years, translation quality has been greatly improved with the emergence of neural machine translation (NMT). In this article, we first review the history of machine translation from rule-based machine translation to example-based machine translation and statistical machine translation. We then introduce NMT in more detail, including the basic framework and the current dominant framework, Transformer, as well as multilingual translation models to deal with the data sparseness problem. In addition, we introduce cutting-edge simultaneous translation methods that achieve a balance between translation quality and latency. We then describe various products and applications of machine translation. At the end of this article, we briefly discuss challenges and future research directions in this field.}
}

@misc{rmae,
      title={RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder}, 
      author={Shitao Xiao and Zheng Liu and Yingxia Shao and Zhao Cao},
      year={2022},
      eprint={2205.12035},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}